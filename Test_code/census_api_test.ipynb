{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import zipfile \n",
    "import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# from census import Census\n",
    "# from us import states\n",
    "# import zipfile \n",
    "# from configy import api_key, g_key\n",
    "# from uszipcode import SearchEngine\n",
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "from geopy.geocoders import GoogleV3\n",
    "from prefect.blocks.system import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/dlzbbxfn6tgfd6s50hkzfk3r0000gn/T/ipykernel_96184/2899126712.py:1: RuntimeWarning: coroutine 'Block.load' was never awaited\n",
      "  api = Secret.load(\"api-key\").get()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m api \u001b[39m=\u001b[39m Secret\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mapi-key\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mget()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "@flow()\n",
    "# def api_demo_to_gcs(year: int, state: str, api_key: str) -> None:\n",
    "#     dataset_state_file = f'state/{year}_states_demographic_data'\n",
    "#     dataset_city_file = f'city/{year}_{state}_city_demographic_data'\n",
    "#     dataset_zip_file = f'zip_code/{year}_zip_demographic_data'\n",
    "\n",
    "#     state_df, city_df, zip_code_df = extract_demographic_data(year, state, api_key)\n",
    "#     state_df, city_df, zip_code_df = transform_demo_data(state_df, city_df, zip_code_df)\n",
    "#     write_demo_to_gcs(state_df, city_df, zip_code_df, dataset_state_file, dataset_city_file, dataset_zip_file)\n",
    "\n",
    "# @flow\n",
    "# def etl_demo_parent_flow() -> None:\n",
    "#     years = list(range(2021, 2015, -1))\n",
    "#     states_list = ['California','Florida','New York','Texas','Pennsylvania']\n",
    "\n",
    "#     [api_demo_to_gcs(year, state, api_key) for state in states_list for year in years]\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     etl_demo_parent_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(log_prints=True, tags=['extracting_RE_data'])\n",
    "def extract_realestate_data() -> str:\n",
    "\n",
    "    !kaggle datasets download -d ahmedshahriarsakib/usa-real-estate-dataset\n",
    "    !unzip usa-real-estate-dataset.zip -d /opt/prefect/flows/data/real_estate/\n",
    "\n",
    "    csv_path = os.path('/data/real_estate', 'realtor-data.csv')\n",
    "\n",
    "    return csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading usa-real-estate-dataset.zip to /Users/og/Desktop/Final_Project/Data-Engineering-Project/Test_code\n",
      " 74%|████████████████████████████          | 11.0M/14.9M [00:00<00:00, 32.9MB/s]\n",
      "100%|██████████████████████████████████████| 14.9M/14.9M [00:00<00:00, 33.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d ahmedshahriarsakib/usa-real-estate-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/real_estate/realtor-data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "zip_path = './usa-real-estate-dataset.zip'\n",
    "csv_folder = '/opt/prefect/flows/data/real_estate'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(csv_folder)\n",
    "\n",
    "os.remove(zip_path)\n",
    "csv_path = os.path.join(csv_folder, 'realtor-data.csv')\n",
    "print(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@task(log_prints=True, tags=['Transformation'])\n",
    "def transform_realestate_data(csv_path: Path, years: list, states: list) -> pd.DataFrame:\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(df.shape)\n",
    "\n",
    "    df['sold_date'] = pd.to_datetime(df['sold_date'], format='%Y-%m-%d')\n",
    "    df = df[df['state'].isin(states)]\n",
    "\n",
    "    years_as_dates = [datetime.datetime(year, 1, 1) for year in years]\n",
    "\n",
    "    df = df[df['sold_date'].between(min(years_as_dates), max(years_as_dates), inclusive=True)]\n",
    "\n",
    "    print(df.shape)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    df[['house_size','bed','bath','zip_code']] = df[['house_size','bed','bath','zip_code']].fillna(0).astype(int)\n",
    "\n",
    "    print(df.dtypes)\n",
    "    print(df.isnull().sum())\n",
    "    os.remove(csv_path)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install uszipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-Levenshtein"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(year: int, state: str, api_key: str) -> pd.DataFrame:\n",
    "    c = Census(api_key, year=year)\n",
    "    variables = [\n",
    "        'NAME',\n",
    "    ]\n",
    "    state_code = states.lookup(state).fips\n",
    "\n",
    "    zip_data = c.acs5.state_zipcode(\n",
    "        variables,\n",
    "        state_fips=state_code,\n",
    "        zcta='*',\n",
    "    )\n",
    "    city_data = c.acs5.state_place(\n",
    "        variables,\n",
    "        state_code,\n",
    "        Census.ALL,\n",
    "    )\n",
    "\n",
    "    state_data = c.acs5.get(\n",
    "        variables,\n",
    "        {'for': 'state:*'},\n",
    "    )\n",
    "    state_df = pd.DataFrame(state_data)\n",
    "    city_df = pd.DataFrame(city_data)\n",
    "    zip_code_df = pd.DataFrame(zip_data)\n",
    "    \n",
    "    city_df = city_df.drop(['state','place'], axis=1)\n",
    "    state_df = state_df.drop('state', axis=1)\n",
    "    zip_code_df = zip_code_df.drop('NAME', axis=1)\n",
    "\n",
    "    state_columns = {'NAME': 'state'}\n",
    "    city_columns = {'NAME': 'city'}\n",
    "    zip_columns = {'zip code tabulation area': 'zip_code'}\n",
    "\n",
    "    state_df = state_df.rename(columns=state_columns)\n",
    "    city_df = city_df.rename(columns=city_columns)\n",
    "    zip_code_df = zip_code_df.rename(columns=zip_columns)\n",
    "\n",
    "    city_df['city'] = city_df['city'].str.split(',', expand=True)[0]\n",
    "    \n",
    "    return state_df, city_df, zip_code_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "state = 'California'\n",
    "\n",
    "state_df, city_df, zip_code_df = extract_coordinates(year, state, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Usage example\u001b[39;00m\n\u001b[1;32m     16\u001b[0m state \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCalifornia\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m coordinates \u001b[39m=\u001b[39m extract_coordinates_by_state(state)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Print the coordinates\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m zipcode, lat, lng \u001b[39min\u001b[39;00m coordinates:\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mextract_coordinates_by_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      7\u001b[0m coordinates \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m zipcode \u001b[39min\u001b[39;00m zipcodes:\n\u001b[0;32m----> 9\u001b[0m     location \u001b[39m=\u001b[39m geolocator\u001b[39m.\u001b[39;49mgeocode(zipcode\u001b[39m.\u001b[39;49mzipcode)\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m location \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         coordinates\u001b[39m.\u001b[39mappend((zipcode\u001b[39m.\u001b[39mzipcode, location\u001b[39m.\u001b[39mlatitude, location\u001b[39m.\u001b[39mlongitude))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/geopy/geocoders/google.py:276\u001b[0m, in \u001b[0;36mGoogleV3.geocode\u001b[0;34m(self, query, exactly_one, timeout, bounds, region, components, place_id, language, sensor)\u001b[0m\n\u001b[1;32m    274\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.geocode: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, url)\n\u001b[1;32m    275\u001b[0m callback \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_json, exactly_one\u001b[39m=\u001b[39mexactly_one)\n\u001b[0;32m--> 276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_geocoder(url, callback, timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/geopy/geocoders/base.py:368\u001b[0m, in \u001b[0;36mGeocoder._call_geocoder\u001b[0;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39mif\u001b[39;00m is_json:\n\u001b[0;32m--> 368\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter\u001b[39m.\u001b[39;49mget_json(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mreq_headers)\n\u001b[1;32m    369\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter\u001b[39m.\u001b[39mget_text(url, timeout\u001b[39m=\u001b[39mtimeout, headers\u001b[39m=\u001b[39mreq_headers)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/geopy/adapters.py:447\u001b[0m, in \u001b[0;36mRequestsAdapter.get_json\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_json\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[0;32m--> 447\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    448\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m         \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/geopy/adapters.py:457\u001b[0m, in \u001b[0;36mRequestsAdapter._request\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, url, \u001b[39m*\u001b[39m, timeout, headers):\n\u001b[1;32m    456\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m         resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url, timeout\u001b[39m=\u001b[39;49mtimeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    458\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    459\u001b[0m         message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(error)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/airflow_env/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_coordinates_by_state(state):\n",
    "    search = SearchEngine()\n",
    "    zipcodes = search.by_state(state, returns=99999)  # Set a high value for returns\n",
    "\n",
    "    geolocator = GoogleV3(api_key=g_key)\n",
    "\n",
    "    coordinates = []\n",
    "    for zipcode in zipcodes:\n",
    "        location = geolocator.geocode(zipcode.zipcode)\n",
    "        if location is not None:\n",
    "            coordinates.append((zipcode.zipcode, location.latitude, location.longitude))\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "# Usage example\n",
    "state = 'California'\n",
    "coordinates = extract_coordinates_by_state(state)\n",
    "\n",
    "# Print the coordinates\n",
    "for zipcode, lat, lng in coordinates:\n",
    "    print(f\"Zip Code: {zipcode}, Latitude: {lat}, Longitude: {lng}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pyspark.sql import SparkSession\n",
    "# import logging\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Create a SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Convert pandas DataFrame to PySpark DataFrame\n",
    "# pyspark_df = spark.createDataFrame(zip_code_df)\n",
    "\n",
    "# # Convert PySpark DataFrame to Pandas DataFrame\n",
    "# pandas_df = pyspark_df.toPandas()\n",
    "\n",
    "# # Add latitude and longitude columns using pandas apply method\n",
    "# def add_coordinates(row):\n",
    "#     zip_code = row['zip_code']\n",
    "#     latitude, longitude = get_coordinates(zip_code, \"\")\n",
    "#     print(f\"Zip Code: {zip_code}, Latitude: {latitude}, Longitude: {longitude}\")\n",
    "#     return pd.Series([latitude, longitude])\n",
    "\n",
    "# pandas_df[['latitude', 'longitude']] = pandas_df.apply(add_coordinates, axis=1)\n",
    "\n",
    "# # Convert Pandas DataFrame back to PySpark DataFrame\n",
    "# pyspark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "# pyspark_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df['latitude'] = \"\"\n",
    "state_df['longitude'] = \"\"\n",
    "for index, row in state_df.iterrows():\n",
    "    name = row['zip_code']\n",
    "    latitude, longitude = get_coordinates(name, \"\")\n",
    "    state_df.at[index, 'latitude'] = latitude\n",
    "    state_df.at[index, 'longitude'] = longitude\n",
    "    print(f\"Coordinates for State {name}: Latitude={latitude}, Longitude={longitude}\")\n",
    "\n",
    "\n",
    "city_df['latitude'] = \"\"\n",
    "city_df['longitude'] = \"\"\n",
    "for index, row in city_df.iterrows():\n",
    "    city = row['city']\n",
    "    latitude, longitude = get_coordinates(city, \"\")\n",
    "    city_df.at[index, 'latitude'] = latitude\n",
    "    city_df.at[index, 'longitude'] = longitude\n",
    "    print(f\"Coordinates for city {city}: Latitude={latitude}, Longitude={longitude}\")\n",
    "\n",
    "\n",
    "zip_code_df['latitude'] = \"\"\n",
    "zip_code_df['longitude'] = \"\"\n",
    "for index, row in zip_code_df.iterrows():\n",
    "    zip_code = row['zip_code']\n",
    "    latitude, longitude = get_coordinates(zip_code, \"\")\n",
    "    zip_code_df.at[index, 'latitude'] = latitude\n",
    "    zip_code_df.at[index, 'longitude'] = longitude\n",
    "    print(f\"Coordinates for zip code {zip_code}: Latitude={latitude}, Longitude={longitude}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Notes\n",
    "- add poverty_rate, employment_rate with pyspark in second ETL pipline to BQ warehouse\n",
    "\n",
    "- split city column into city & state with PD first ETL pipeline to Storage lake \n",
    "- zip_df remove zip_code_tabluation_area first ETL pipeline with PD\n",
    "- retrieve longitiude and latitude for zip, city, state level then add column and concat with PD\n",
    "- drop 'place' column from city_df with PD\n",
    "- reindex columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Columns: {state_df.dtypes}')\n",
    "# print(f'Rows: {len(state_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_rates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df[\"poverty_rate\"] = 100 * df['poverty_count'].astype(int) / df[\"population\"].astype(int)\n",
    "#     df[\"unemployment_rate\"] = 100 * df[\"unemployment_count\"].astype(int) / df[\"population\"].astype(int)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# state_df = calculate_rates(state_df)\n",
    "# city_df = calculate_rates(city_df)\n",
    "# zip_code_df = calculate_rates(zip_code_df)\n",
    "\n",
    "# state_df = state_df[['year', 'name', 'population', 'median_age', 'household_income', 'per_capita_income',\n",
    "#                     'poverty_count', 'unemployment_count', 'employment_count', 'poverty_rate','unemployment_rate']]\n",
    "# city_df = city_df[['year', 'city', 'population', 'median_age', 'household_income', 'per_capita_income',\n",
    "#                 'poverty_count', 'unemployment_count', 'employment_count', 'poverty_rate', 'unemployment_rate']]\n",
    "# zip_code_df = zip_code_df[['year', 'zip_code', 'population', 'median_age', 'household_income', 'per_capita_income',\n",
    "#                         'poverty_count', 'unemployment_count', 'employment_count', 'poverty_rate', 'unemployment_rate']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State economic data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Economic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip Code Economic Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = zip_code_df['geographical_mobility'].isnull().sum()\n",
    "# values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms - Geographic\n",
    "- reorder index\n",
    "    - year first, location second\n",
    "- drop zip code tabulation area' \n",
    "- drop first half of zip code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Demographic Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "- Reorder year, place to start of df\n",
    "- remove state # in state_df & city_df\n",
    "- split city, and zip code column "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
